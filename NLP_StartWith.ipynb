{"cells":[{"cell_type":"code","source":["import findspark"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["import pyspark"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["from pyspark.sql import SparkSession"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["spark = SparkSession.builder.appName('NLP_starter').getOrCreate()"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["from pyspark.ml.feature import Tokenizer, RegexTokenizer"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["from pyspark.sql.functions import udf "],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["from pyspark.sql.types import IntegerType"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["rdd = sc.textFile('/FileStore/tables/Purelyrics.txt/part-00000')"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["rdd.take(10)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["rdd1 = rdd.zipWithIndex()\nrdd2 = rdd1.map(lambda s:(s[1],s[0]))\nrdd2.take(10)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["from pyspark.sql.types import *\nschema = StructType([StructField('id', IntegerType(), True),StructField('lyrics', StringType(), True)])\nfrom pyspark.sql import SQLContext\n# create dataframe\ndf = sqlContext.createDataFrame(rdd2, schema)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["df.show(5)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["tokenizer = Tokenizer(inputCol='lyrics', outputCol='words')"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["## More on regular expressions with Python\nMore on:\nhttps://docs.python.org/3/library/re.html\n\n### \\W\nMatches any character which is not a word character. This is the opposite of \\w. If the ASCII flag is used this becomes the equivalent of [^a-zA-Z0-9_] (but the flag affects the entire regular expression, so in such cases using an explicit [^a-zA-Z0-9_] may be a better choice)."],"metadata":{}},{"cell_type":"code","source":["count_words = udf(lambda words: len(words), IntegerType())"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["tokenized_df = tokenizer.transform(df)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["tokenized_df.show(truncate=False)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["tokenized_output = tokenized_df.collect()"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["for token in tokenized_output[4]:\n    print (token)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["tokenized_df.head()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["tokenized_df.withColumn('counts', count_words('words')).show()"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["regex_tokenizer = RegexTokenizer(inputCol='lyrics', outputCol='words', pattern='\\\\W')\n#regex_tokenizer.setMinTokenLength(4)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["regex_df = regex_tokenizer.transform(df)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["regex_tokenized_counts = regex_df.withColumn('freq', count_words('words'))"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["regex_tokenized_counts.show()"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["from pyspark.ml.feature import StopWordsRemover"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["remover = StopWordsRemover(inputCol='words', outputCol='tokens')"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["tokens_filtered = remover.transform(regex_tokenized_counts)"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["cleanDF= tokens_filtered.withColumn('count_tokens', count_words('tokens'))\n"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["cleanDF.select('words', 'freq', 'tokens', 'count_tokens').show()"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["cleanDF.show()"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["remover.getStopWords()"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["for item in cleanDF.collect()[4]:\n   print(item)    "],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["stopWords=['a', 'is', 'for', 'hi', 'in', 'on','row','lyrics','u']\nremover.setStopWords(stopWords)\n"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["remover.transform(regex_tokenized_counts).select('lyrics', 'tokens').show(truncate=False)"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["newCleanDF=remover.transform(regex_tokenized_counts).withColumn('count_tokens', count_words('tokens'))\nnewCleanDF.show()\n"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["from pyspark.ml.feature import NGram"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["ngram = NGram(n=2, inputCol='tokens', outputCol='2grams')"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["#my_2ngrams =ngram.transform(cleanDF)\nmy_2ngrams =ngram.transform(newCleanDF)"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":[" my_2ngrams.show()"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["my_2ngrams.select('2grams').show(truncate =False)"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["from pyspark.ml.feature import HashingTF, IDF"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["cleanDF.show()"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["tf = HashingTF(inputCol='tokens', outputCol='features')"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":["# TF: Term Frequency\nMaps a sequence of terms to their term frequencies using the hashing trick.\n\nNote: the terms must be hashable (can not be dictionary or list...).\n\nHashingTF(S) takes the hash code of each word modulo the desired vector size S, and thus maps each word to a number between 0 and S-1.\n\nThis yields a quite robust vector even if multiple words may map to the same hash code. \n\nThe MLib developers recommend setting S between 2^18 and 2^20"],"metadata":{}},{"cell_type":"code","source":["tf.explainParams()\n"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["print (tf.getNumFeatures())"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["tf_df = tf.transform(newCleanDF)"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"code","source":["tf_df.show()"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"code","source":["idf = IDF(inputCol='features', outputCol='idf_features')"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"markdown","source":["# TF-IDF: Term Frequence - Inverse Document Frequency\n\nOnce you have TF vectors, you can use IDF to compute the inverse document frequencies and multiply them with the TF to compute the TF-IDF \n\nIDF measures how infrequently a term occurs across the whole document corpus\n\nTF x IDF shows how relevant a term is to specific document (i.e., if it is common in that document but rare in the whole corpus)\n\nTF-IDF is used to improve on Bag of Words by adjusting word counts based on their frequency in the corpus\n\n## How to calculate them?\nVarious ways for determining the exact values of both statistics exist:\n\n- TF(x, y): number of occurences of term x in document y. It represents the importance of a term in the document. \n\n- IDF(t): Importance of the term in the document. \n\n- IDF(t)= log(N/N(t))\n- N: number of documents in the corpus D, N=|D|\n- N(t): Number of the documents where the term t appears (i.e: TF(t, d)!=0). N(t)= |{d in D, t in D}|\n- TF-IDF(t, d) =TF(t, d)  \n\n## Further Information on TF-IDF\nhttps://fr.wikipedia.org/wiki/TF-IDF\n\n## Preparing Data to TF-IDF\nIn a real pipeline, you will likely need to preprocess and stem words before passing them to TF.\n\nEx: convert words to lowercase, drop punctuation characters or drop suffixes like ‘ing’.\n\nYou can use external single node natural language libraries like NLTK (http://www.nltk.org)"],"metadata":{}},{"cell_type":"code","source":["idf_model = idf.fit(tf_df)"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"code","source":["data=idf_model.transform(tf_df)"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":["data.show()"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"code","source":["from pyspark.ml.feature import CountVectorizer"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"code","source":["count_vec = CountVectorizer(inputCol='tokens', outputCol='features',  minDF=1)"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"code","source":["#help(   CountVectorizer)"],"metadata":{"collapsed":true},"outputs":[],"execution_count":57},{"cell_type":"code","source":["model = count_vec.fit(newCleanDF)"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"code","source":["data = model.transform(newCleanDF)"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"code","source":["data.select(['tokens', 'features']).show(truncate = False)"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"code","source":["data.rdd.saveAsTextFile('/FileStore/tables/feature')"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"code","source":["count_vec = CountVectorizer(inputCol='tokens', outputCol='features', minDF=2)"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"code","source":["count_vec.fit(cleanDF).transform(cleanDF).select('tokens', 'features').show(truncate = False)"],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"code","source":["count_vec = CountVectorizer(inputCol='tokens', outputCol='features', vocabSize=15, minDF=1)"],"metadata":{},"outputs":[],"execution_count":64},{"cell_type":"code","source":["count_vec.fit(cleanDF).transform(cleanDF).select('tokens', 'features').show(truncate = False)"],"metadata":{},"outputs":[],"execution_count":65}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.5.2","nbconvert_exporter":"python","file_extension":".py"},"name":"NLP_StartWith","notebookId":3534009686211371},"nbformat":4,"nbformat_minor":0}

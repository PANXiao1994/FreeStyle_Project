{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from pyspark import SparkContext\n",
    "from nltk.stem import SnowballStemmer\n",
    "from tokenize import tokenize\n",
    "from nltk.corpus import stopwords,words\n",
    "from scipy.spatial.distance import cdist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "def token_processor(token):\n",
    "    return stemmer.stem(token)\n",
    "wordss = words.words()\n",
    "\n",
    "def closest_node(node, nodes):\n",
    "    return nodes[cdist([node], nodes).argmin()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding:utf8\n",
    "from pyspark import SparkContext\n",
    "from pyspark import RDD\n",
    "import numpy as np\n",
    "from numpy.random import RandomState\n",
    "\n",
    "import sys\n",
    "if sys.version[0] == '2':\n",
    "    reload(sys)\n",
    "    sys.setdefaultencoding(\"utf-8\")\n",
    "\n",
    "\n",
    "\n",
    "class PLSA:\n",
    "\n",
    "    def __init__(self, data, sc, k, is_test=False, max_itr=1000, eta=1e-6):\n",
    "\n",
    "        \"\"\"\n",
    "        init the algorithm\n",
    "\n",
    "        :type data RDD\n",
    "        :param data: document rdd\n",
    "        :type max_itr int\n",
    "        :param max_itr: maximum EM iter\n",
    "        :type is_test bool\n",
    "        :param is_test: test or not,if yes, rd = RandomState(1)，otherwise rd = RandomState()\n",
    "        :type sc SparkContext\n",
    "        :param sc: spark context\n",
    "        :type k int\n",
    "        :param k : number of theme\n",
    "        :type eta float\n",
    "        :param : threshold，when the changement of log likelyhood<eta, stop iteration\n",
    "        :return : PLSA object\n",
    "        \"\"\"\n",
    "\n",
    "        self.max_itr = max_itr\n",
    "        self.k = sc.broadcast(k)\n",
    "        self.ori_data = data#.map(lambda x: x.split(' '))\n",
    "        self.data = data\n",
    "        self.sc = sc\n",
    "        self.eta = eta\n",
    "        self.rd = sc.broadcast(RandomState(1) if is_test else RandomState())\n",
    "\n",
    "    def train(self):\n",
    "        #get the dictionary words\n",
    "        self.word_dict_b = self._init_dict_()\n",
    "        #transform the words in the documents into the indexes in the dictionary\n",
    "        self._convert_docs_to_word_index()\n",
    "        #initialization, the distribution under each theme\n",
    "        self._init_probility_word_topic_()\n",
    "\n",
    "        pre_l= self._log_likelyhood_()\n",
    "\n",
    "        print(\"L(%d)=%.5f\" %(0,pre_l))\n",
    "\n",
    "        for i in range(self.max_itr):\n",
    "            #update the posterior distribution\n",
    "            self._E_step_()\n",
    "            #maimize the lower bound\n",
    "            self._M_step_()\n",
    "            now_l = self._log_likelyhood_()\n",
    "\n",
    "            improve = np.abs((pre_l-now_l)/pre_l)\n",
    "            pre_l = now_l\n",
    "\n",
    "            print(\"L(%d)=%.5f with %.6f%% improvement\" %(i+1,now_l,improve*100))\n",
    "            if improve <self.eta:\n",
    "                break\n",
    "\n",
    "    def _M_step_(self):\n",
    "        \"\"\"\n",
    "        update: p(z=k|d),p(w|z=k)\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        k = self.k\n",
    "        v = self.v\n",
    "\n",
    "        def update_probility_of_doc_topic(doc):\n",
    "            \"\"\"\n",
    "            update the distribution of the documents of the themes\n",
    "            \"\"\"\n",
    "            topic_doc = doc['topic'] - doc['topic']\n",
    "            words = doc['words']\n",
    "            for (word_index,word) in words.items():\n",
    "                topic_doc += word['count']*word['topic_word']\n",
    "            topic_doc /= np.sum(topic_doc)\n",
    "\n",
    "            return {'words':words,'topic':topic_doc}\n",
    "\n",
    "        self.data = self.data.map(update_probility_of_doc_topic)\n",
    "        \n",
    "        self.data.cache()\n",
    "\n",
    "        def update_probility_word_given_topic(doc):\n",
    "            \"\"\"\n",
    "            up date the distribution of the words of the themes\n",
    "            \"\"\"\n",
    "            probility_word_given_topic = np.matrix(np.zeros((k.value,v.value)))\n",
    "\n",
    "            words = doc['words']\n",
    "            for (word_index,word) in words.items():\n",
    "                probility_word_given_topic[:,word_index] += np.matrix(word['count']*word['topic_word']).T\n",
    "\n",
    "            return probility_word_given_topic\n",
    "\n",
    "        probility_word_given_topic = self.data.map(update_probility_word_given_topic).sum()\n",
    "        probility_word_given_topic_row_sum = np.matrix(np.sum(probility_word_given_topic,axis=1))\n",
    "\n",
    "        #normalization\n",
    "        probility_word_given_topic = np.divide(probility_word_given_topic,probility_word_given_topic_row_sum)\n",
    "\n",
    "        self.probility_word_given_topic = self.sc.broadcast(probility_word_given_topic)\n",
    "\n",
    "    def _E_step_(self):\n",
    "        \"\"\"\n",
    "        update the latent viariable:  p(z|w,d)\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        probility_word_given_topic = self.probility_word_given_topic\n",
    "        k = self.k\n",
    "\n",
    "        def update_probility_of_word_topic_given_word(doc):\n",
    "            topic_doc = doc['topic']\n",
    "            words = doc['words']\n",
    "\n",
    "            for (word_index,word) in words.items():\n",
    "                topic_word = word['topic_word']\n",
    "                for i in range(k.value):\n",
    "                    topic_word[i] = probility_word_given_topic.value[i,word_index]*topic_doc[i]\n",
    "                #normalization\n",
    "                topic_word /= np.sum(topic_word)\n",
    "                word['topic_word'] = topic_word # added\n",
    "            return {'words':words,'topic':topic_doc}\n",
    "\n",
    "        self.data = self.data.map(update_probility_of_word_topic_given_word)\n",
    "\n",
    "    def  _init_probility_word_topic_(self):\n",
    "        \"\"\"\n",
    "        init p(w|z=k)\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        #dict length(words in dict)\n",
    "        m = self.v.value\n",
    "\n",
    "        probility_word_given_topic = self.rd.value.uniform(0,1,(self.k.value,m))\n",
    "        probility_word_given_topic_row_sum = np.matrix(np.sum(probility_word_given_topic,axis=1)).T\n",
    "\n",
    "        #normalization\n",
    "        probility_word_given_topic = np.divide(probility_word_given_topic,probility_word_given_topic_row_sum)\n",
    "\n",
    "        self.probility_word_given_topic = self.sc.broadcast(probility_word_given_topic)\n",
    "\n",
    "    def _convert_docs_to_word_index(self):\n",
    "\n",
    "        word_dict_b = self.word_dict_b\n",
    "        k = self.k\n",
    "        rd = self.rd\n",
    "        '''\n",
    "        I wonder is there a better way to execute function with broadcast varible\n",
    "        '''\n",
    "        def _word_count_doc_(doc):\n",
    "            print(doc)\n",
    "            wordcount ={}\n",
    "            word_dict = word_dict_b.value\n",
    "            for word in doc:\n",
    "                if word_dict[word] in wordcount:\n",
    "                    wordcount[word_dict[word]]['count'] += 1\n",
    "                else:\n",
    "                    #first one is the number of word occurance\n",
    "                    #second one is p(z=k|w,d)\n",
    "                    wordcount[word_dict[word]] = {'count':1,'topic_word': rd.value.uniform(0,1,k.value)}\n",
    "\n",
    "            topics = rd.value.uniform(0, 1, k.value)\n",
    "            topics = topics/np.sum(topics)\n",
    "            return {'words':wordcount,'topic':topics}\n",
    "        self.data = self.ori_data.map(_word_count_doc_)\n",
    "\n",
    "    def _init_dict_(self):\n",
    "        \"\"\"\n",
    "        init word dict of the documents,\n",
    "        and broadcast it\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        words = self.ori_data.flatMap(lambda d: d).distinct().collect()\n",
    "        word_dict = {w: i for w, i in zip(words, range(len(words)))}\n",
    "        self.v = self.sc.broadcast(len(word_dict))\n",
    "        return self.sc.broadcast(word_dict)\n",
    "\n",
    "    def _log_likelyhood_(self):\n",
    "        \n",
    "        probility_word_given_topic = self.probility_word_given_topic\n",
    "        k = self.k\n",
    "        def likelyhood(doc):\n",
    "            print(\"succ\")\n",
    "            l = 0.0\n",
    "            topic_doc = doc['topic']\n",
    "            words = doc['words']\n",
    "            for (word_index,word) in words.items():\n",
    "                print(word)\n",
    "                l += word['count']*np.log(np.matrix(topic_doc)*probility_word_given_topic.value[:,word_index])\n",
    "            return l\n",
    "        return self.data.map(likelyhood).sum()\n",
    "\n",
    "\n",
    "\n",
    "    def save(self):\n",
    "        \"\"\"\n",
    "        save the result of the model TODO \n",
    "        :param f_word_given_topic: distribution of words given the topic\n",
    "        :param f_doc_topic:  distribution of topic given the documents\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        doc_topic = self.data.map(lambda x:' '.join([str(q) for q in x['topic'].tolist()])).collect()\n",
    "        probility_word_given_topic = self.probility_word_given_topic.value\n",
    "\n",
    "        word_dict = self.word_dict_b.value\n",
    "        word_given_topic = []\n",
    "\n",
    "        for w,i in word_dict.items():\n",
    "            word_given_topic.append('%s %s' %(w,' '.join([str(q[0]) for q in probility_word_given_topic[:,i].tolist()])))\n",
    "        return word_given_topic, doc_topic\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data = sc.textFile(\"/Users/panxiao/IdeaProjects/lyric_project/input/lyrics.csv\")\n",
    "# data = sc.textFile(\"../lyr.txt\")\n",
    "# data1 = sc.parallelize(data.take(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data2 = data1.map(lambda x: re.sub(r\"\\W+\",\" \", str(x).lower()))\n",
    "# data3 = data2.map(lambda x: x.split(' '))\n",
    "# data4 = data3.map(lambda l: [x for x in l if x and x in wordss and x not in stopwords.words(\"english\")])\n",
    "# data5 = data4.map(lambda l: \" \".join(x for x in l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plsa = PLSA(data=data4,sc=sc,k=5,max_itr=10,is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L(0)=-47322.77605\n",
      "L(1)=-39816.96600 with 15.860883% improvement\n",
      "L(2)=-39199.63764 with 1.550415% improvement\n",
      "L(3)=-38320.42535 with 2.242909% improvement\n",
      "L(4)=-37257.51865 with 2.773734% improvement\n",
      "L(5)=-36328.59207 with 2.493259% improvement\n",
      "L(6)=-35716.79729 with 1.684059% improvement\n",
      "L(7)=-35355.76814 with 1.010811% improvement\n",
      "L(8)=-35130.30709 with 0.637692% improvement\n",
      "L(9)=-34976.52296 with 0.437753% improvement\n",
      "L(10)=-34864.82935 with 0.319339% improvement\n"
     ]
    }
   ],
   "source": [
    "plsa.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_given_topic, topic_given_doc = plsa.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word = pd.DataFrame([sub.split(\" \") for sub in word_given_topic])\n",
    "topic_word_1 = topic_word[topic_word.columns[:6]]\n",
    "topic_word_1 = topic_word_1.set_index(0).astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_topic = pd.DataFrame([sub.split(\" \") for sub in topic_given_doc])\n",
    "doc_topic_1 = doc_topic[doc_topic.columns[:6]]\n",
    "doc_topic_1 = doc_topic_1.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.552794e-01</td>\n",
       "      <td>0.134975</td>\n",
       "      <td>0.001326</td>\n",
       "      <td>0.007109</td>\n",
       "      <td>0.001310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.457515e-01</td>\n",
       "      <td>0.174097</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.011834</td>\n",
       "      <td>0.568211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.035068e-03</td>\n",
       "      <td>0.228032</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.707619</td>\n",
       "      <td>0.061272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.440765e-02</td>\n",
       "      <td>0.094287</td>\n",
       "      <td>0.711033</td>\n",
       "      <td>0.167938</td>\n",
       "      <td>0.002334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.117353e-07</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.999961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.038094e-05</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.006569</td>\n",
       "      <td>0.057533</td>\n",
       "      <td>0.935648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.357306e-04</td>\n",
       "      <td>0.014669</td>\n",
       "      <td>0.067429</td>\n",
       "      <td>0.012144</td>\n",
       "      <td>0.905123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.373357e-05</td>\n",
       "      <td>0.009437</td>\n",
       "      <td>0.968590</td>\n",
       "      <td>0.003125</td>\n",
       "      <td>0.018834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5.805301e-03</td>\n",
       "      <td>0.020588</td>\n",
       "      <td>0.007185</td>\n",
       "      <td>0.957579</td>\n",
       "      <td>0.008844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8.580650e-01</td>\n",
       "      <td>0.007072</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.134340</td>\n",
       "      <td>0.000476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4\n",
       "0  8.552794e-01  0.134975  0.001326  0.007109  0.001310\n",
       "1  2.457515e-01  0.174097  0.000107  0.011834  0.568211\n",
       "2  3.035068e-03  0.228032  0.000042  0.707619  0.061272\n",
       "3  2.440765e-02  0.094287  0.711033  0.167938  0.002334\n",
       "4  4.117353e-07  0.000007  0.000003  0.000028  0.999961\n",
       "5  5.038094e-05  0.000199  0.006569  0.057533  0.935648\n",
       "6  6.357306e-04  0.014669  0.067429  0.012144  0.905123\n",
       "7  1.373357e-05  0.009437  0.968590  0.003125  0.018834\n",
       "8  5.805301e-03  0.020588  0.007185  0.957579  0.008844\n",
       "9  8.580650e-01  0.007072  0.000046  0.134340  0.000476"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topic_1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word_1.sort_values(topic_word_1.columns[4],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word.sort_values(topic_word.columns[2],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word.sort_values(topic_word.columns[3],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word.sort_values(topic_word.columns[4],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word.sort_values(topic_word.columns[5],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_given_doc_1 = [x.split(\" \") for x in topic_given_doc]\n",
    "topic_given_doc_2 = [[float(y) for y in x]for x in topic_given_doc_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "some_pt = topic_given_doc_2[0]\n",
    "min_index = distance.cdist([some_pt], topic_given_doc_2)[0].argsort()\n",
    "#topic_given_doc_2[min_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0 21  9 32 11 28 10 44 46 33 25 36  1 26 12 35  3 34  2 16 29 23 48 30 18\n",
      " 38 45 37 17 43  6 40 14  5 42  8 15 41 39  7 13 27 19 20 22 47 49  4 31 24]\n"
     ]
    }
   ],
   "source": [
    "print(min_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "print(len(topic_given_doc_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

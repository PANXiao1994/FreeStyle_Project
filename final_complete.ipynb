{"cells":[{"cell_type":"markdown","source":["## PLSA"],"metadata":{}},{"cell_type":"code","source":["import re\nimport numpy as np\nimport pandas as pd\n#import nltk\nfrom pyspark import SparkContext\n#from nltk.stem import SnowballStemmer\n#from tokenize import tokenize\n#from nltk.corpus import stopwords,words\nfrom scipy.spatial.distance import cdist\nimport pronouncing\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.sql.functions import col, split, explode, udf\nfrom pyspark.sql.types import ArrayType, StringType, IntegerType\nfrom pyspark.sql import Row\nfrom pyspark.sql import functions as F\nimport pandas as pd\n\n\n\nrowData = spark.read.csv(\"/FileStore/tables/hadoop_final_project/lyrics.csv\", inferSchema=True, header = True, multiLine=True)\nrowData = rowData.filter(rowData.genre == 'Hip-Hop')\nrowData.show(n=5)\ndata_array_lyrics = rowData.withColumn(\n    \"lyrics\",\n    split(col(\"lyrics\"), \"\\n\").cast(ArrayType(StringType())).alias(\"lyrics\")\n)\ndata_array_lyrics.show(n=5)\ndata_line = data_array_lyrics.withColumn(\"lyrics\", explode(data_array_lyrics.lyrics))\ndata_line.show(n=5)\ndef phone_phrase(phrase):\n  phone_consonant = ['B', 'CH', 'D', 'DH', 'F', 'G', 'HH', 'JH', 'K', 'L', 'M', 'N', 'P', 'R', 'S', 'SH', 'T', 'TH', 'V', 'W', 'Y', 'Z', 'ZH']\n  s = filter(bool, re.split(r'\\W+', phrase.lower()))\n  phone = []\n  for p in s:\n    tmp = pronouncing.phones_for_word(p)\n    if len(tmp) != 0:\n      tmp_list = tmp[0].split()\n      for x in tmp_list:\n        if x not in phone_consonant:\n          phone.append(x[:-1])\n  if len(phone) >=3:\n    res = phone[-3] + phone[-2] + phone[-1]\n  else:\n    res = ''\n  return res\npp_udf = udf(phone_phrase, StringType())\n\ndata_phone = data_line.withColumn(\"phoneme\", pp_udf(data_line['lyrics']))\ndata_phone.show(n=20)\ndef length_phrase(phrase):\n  s = filter(bool, re.split(r'\\W+', phrase.lower()))\n  return len(s)\nlen_udf = udf(length_phrase, IntegerType())\ndata_phone_len = data_phone.withColumn('length', len_udf(data_phone['lyrics']))\ndata_phone_len.show(n=20)\ndata_phone_len.printSchema()\ntest_lyric = 'call me rap king of underground'\npho, length = phone_phrase(test_lyric), length_phrase(test_lyric)\nprint pho, length\nprint type(pho)\ndata_final = data_phone_len.filter(data_phone_len.phoneme == pho).filter(data_phone_len.length<length+5).filter(data_phone_len.length>length-5)\ndata_final = data_final.select(data_final.lyrics).distinct()\ndata_final.show(n=100, truncate=False)\n"],"metadata":{"collapsed":true,"scrolled":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":["import findspark\nimport pyspark\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('NLP_starter').getOrCreate()\nfrom pyspark.ml.feature import Tokenizer, RegexTokenizer\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import IntegerType\nrdd = sc.textFile('/FileStore/tables/Purelyrics.txt/part-00000')\nrdd1 = rdd.zipWithIndex()\nrdd2 = rdd1.map(lambda s:(s[1],s[0]))\n\nfrom pyspark.sql.types import *\nschema = StructType([StructField('id', IntegerType(), True),StructField('lyrics', StringType(), True)])\nfrom pyspark.sql import SQLContext\n# create dataframe\nrawdf = sqlContext.createDataFrame(rdd2, schema)\ndf = sc.parallelize(rawdf.take(500)).toDF()\n\ntokenizer = Tokenizer(inputCol='lyrics', outputCol='words')\ncount_words = udf(lambda words: len(words), IntegerType())\ntokenized_df = tokenizer.transform(df)\n\nregex_tokenizer = RegexTokenizer(inputCol='lyrics', outputCol='words', pattern='\\\\W')\n#regex_tokenizer.setMinTokenLength(4)\nregex_df = regex_tokenizer.transform(df)\nregex_tokenized_counts = regex_df.withColumn('freq', count_words('words'))\nregex_tokenized_counts.show(truncate=False)\n\nfrom pyspark.ml.feature import StopWordsRemover\nremover = StopWordsRemover(inputCol='words', outputCol='tokens')\nstopWords = ['a','an','the', 'is','are', 'for', 'hi', 'in', 'on','row','lyrics','u','t','s','re','i','m']\nremover.setStopWords(stopWords)\ntokens_filtered = remover.transform(regex_tokenized_counts)\ncleanDF= tokens_filtered.withColumn('count_tokens', count_words('tokens'))\n\n#tokens_filtered1 = remover.transform(newcleanDF)\n\n\nfrom pyspark.ml.feature import NGram\nngram = NGram(n=2, inputCol='tokens', outputCol='2grams')\nmy_2ngrams =ngram.transform(cleanDF)\nmy_2ngrams.show()\nmy_2ngrams.select('2grams').show(truncate =False)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["types = [f.dataType for f in my_2ngrams.schema.fields]\ntypes"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["grams = my_2ngrams.select('2grams').collect()"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["gramlist = [list(g[0]) for g in grams]"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["gramrdd = sc.parallelize(gramlist)\ngramrdd.collect()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# coding:utf8\nfrom pyspark import SparkContext\nfrom pyspark import RDD\nimport numpy as np\nfrom numpy.random import RandomState\n\nimport sys\nif sys.version[0] == '2':\n    reload(sys)\n    sys.setdefaultencoding(\"utf-8\")\n\n\n\nclass PLSA:\n\n    def __init__(self, data, sc, k, is_test=False, max_itr=1000, eta=1e-6):\n\n        \"\"\"\n        init the algorithm\n\n        :type data RDD\n        :param data: document rdd\n        :type max_itr int\n        :param max_itr: maximum EM iter\n        :type is_test bool\n        :param is_test: test or not,if yes, rd = RandomState(1)，otherwise rd = RandomState()\n        :type sc SparkContext\n        :param sc: spark context\n        :type k int\n        :param k : number of theme\n        :type eta float\n        :param : threshold，when the changement of log likelyhood<eta, stop iteration\n        :return : PLSA object\n        \"\"\"\n\n        self.max_itr = max_itr\n        self.k = sc.broadcast(k)\n        self.ori_data = data#.map(lambda x: x.split(' '))\n        self.data = data\n        self.sc = sc\n        self.eta = eta\n        self.rd = sc.broadcast(RandomState(1) if is_test else RandomState())\n\n    def train(self):\n        #get the dictionary words\n        self.word_dict_b = self._init_dict_()\n        #transform the words in the documents into the indexes in the dictionary\n        self._convert_docs_to_word_index()\n        #initialization, the distribution under each theme\n        self._init_probility_word_topic_()\n\n        pre_l= self._log_likelyhood_()\n\n        print(\"L(%d)=%.5f\" %(0,pre_l))\n\n        for i in range(self.max_itr):\n            #update the posterior distribution\n            self._E_step_()\n            #maimize the lower bound\n            self._M_step_()\n            now_l = self._log_likelyhood_()\n\n            improve = np.abs((pre_l-now_l)/pre_l)\n            pre_l = now_l\n\n            print(\"L(%d)=%.5f with %.6f%% improvement\" %(i+1,now_l,improve*100))\n            if improve <self.eta:\n                break\n\n    def _M_step_(self):\n        \"\"\"\n        update: p(z=k|d),p(w|z=k)\n        :return: None\n        \"\"\"\n        k = self.k\n        v = self.v\n\n        def update_probility_of_doc_topic(doc):\n            \"\"\"\n            update the distribution of the documents of the themes\n            \"\"\"\n            topic_doc = doc['topic'] - doc['topic']\n            words = doc['words']\n            for (word_index,word) in words.items():\n                topic_doc += word['count']*word['topic_word']\n            topic_doc /= np.sum(topic_doc)\n\n            return {'words':words,'topic':topic_doc}\n\n        self.data = self.data.map(update_probility_of_doc_topic)\n        \n        self.data.cache()\n\n        def update_probility_word_given_topic(doc):\n            \"\"\"\n            up date the distribution of the words of the themes\n            \"\"\"\n            probility_word_given_topic = np.matrix(np.zeros((k.value,v.value)))\n\n            words = doc['words']\n            for (word_index,word) in words.items():\n                probility_word_given_topic[:,word_index] += np.matrix(word['count']*word['topic_word']).T\n\n            return probility_word_given_topic\n\n        probility_word_given_topic = self.data.map(update_probility_word_given_topic).sum()\n        probility_word_given_topic_row_sum = np.matrix(np.sum(probility_word_given_topic,axis=1))\n\n        #normalization\n        probility_word_given_topic = np.divide(probility_word_given_topic,probility_word_given_topic_row_sum)\n\n        self.probility_word_given_topic = self.sc.broadcast(probility_word_given_topic)\n\n    def _E_step_(self):\n        \"\"\"\n        update the latent viariable:  p(z|w,d)\n        :return: None\n        \"\"\"\n        probility_word_given_topic = self.probility_word_given_topic\n        k = self.k\n\n        def update_probility_of_word_topic_given_word(doc):\n            topic_doc = doc['topic']\n            words = doc['words']\n\n            for (word_index,word) in words.items():\n                topic_word = word['topic_word']\n                for i in range(k.value):\n                    topic_word[i] = probility_word_given_topic.value[i,word_index]*topic_doc[i]\n                #normalization\n                topic_word /= np.sum(topic_word)\n                word['topic_word'] = topic_word # added\n            return {'words':words,'topic':topic_doc}\n\n        self.data = self.data.map(update_probility_of_word_topic_given_word)\n\n    def  _init_probility_word_topic_(self):\n        \"\"\"\n        init p(w|z=k)\n        :return: None\n        \"\"\"\n        #dict length(words in dict)\n        m = self.v.value\n\n        probility_word_given_topic = self.rd.value.uniform(0,1,(self.k.value,m))\n        probility_word_given_topic_row_sum = np.matrix(np.sum(probility_word_given_topic,axis=1)).T\n\n        #normalization\n        probility_word_given_topic = np.divide(probility_word_given_topic,probility_word_given_topic_row_sum)\n\n        self.probility_word_given_topic = self.sc.broadcast(probility_word_given_topic)\n\n    def _convert_docs_to_word_index(self):\n\n        word_dict_b = self.word_dict_b\n        k = self.k\n        rd = self.rd\n        '''\n        I wonder is there a better way to execute function with broadcast varible\n        '''\n        def _word_count_doc_(doc):\n            print(doc)\n            wordcount ={}\n            word_dict = word_dict_b.value\n            for word in doc:\n                if word_dict[word] in wordcount:\n                    wordcount[word_dict[word]]['count'] += 1\n                else:\n                    #first one is the number of word occurance\n                    #second one is p(z=k|w,d)\n                    wordcount[word_dict[word]] = {'count':1,'topic_word': rd.value.uniform(0,1,k.value)}\n\n            topics = rd.value.uniform(0, 1, k.value)\n            topics = topics/np.sum(topics)\n            return {'words':wordcount,'topic':topics}\n        self.data = self.ori_data.map(_word_count_doc_)\n\n    def _init_dict_(self):\n        \"\"\"\n        init word dict of the documents,\n        and broadcast it\n        :return: None\n        \"\"\"\n        words = self.ori_data.flatMap(lambda d: d).distinct().collect()\n        word_dict = {w: i for w, i in zip(words, range(len(words)))}\n        self.v = self.sc.broadcast(len(word_dict))\n        return self.sc.broadcast(word_dict)\n\n    def _log_likelyhood_(self):\n        \n        probility_word_given_topic = self.probility_word_given_topic\n        k = self.k\n        def likelyhood(doc):\n            print(\"succ\")\n            l = 0.0\n            topic_doc = doc['topic']\n            words = doc['words']\n            for (word_index,word) in words.items():\n                print(word)\n                l += word['count']*np.log(np.matrix(topic_doc)*probility_word_given_topic.value[:,word_index])\n            return l\n        return self.data.map(likelyhood).sum()\n\n\n\n    def save(self):\n        \"\"\"\n        save the result of the model TODO \n        :param f_word_given_topic: distribution of words given the topic\n        :param f_doc_topic:  distribution of topic given the documents\n        :return:\n        \"\"\"\n        doc_topic = self.data.map(lambda x:' '.join([str(q) for q in x['topic'].tolist()])).collect()\n        probility_word_given_topic = self.probility_word_given_topic.value\n\n        word_dict = self.word_dict_b.value\n        word_given_topic = []\n\n        for w,i in word_dict.items():\n            word_given_topic.append('%s %s' %(w,' '.join([str(q[0]) for q in probility_word_given_topic[:,i].tolist()])))\n        return word_given_topic, doc_topic\n    "],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["# sc = SparkContext()"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["#data = sc.textFile(\"/Users/panxiao/IdeaProjects/lyric_project/input/lyrics.csv\")\n# data = sc.textFile(\"../lyr.txt\")\n# data1 = sc.parallelize(data.take(50))"],"metadata":{"scrolled":true},"outputs":[],"execution_count":10},{"cell_type":"code","source":["# data2 = data1.map(lambda x: re.sub(r\"\\W+\",\" \", str(x).lower()))\n# data3 = data2.map(lambda x: x.split(' '))\n# data4 = data3.map(lambda l: [x for x in l if x and x in wordss and x not in stopwords.words(\"english\")])\n# data5 = data4.map(lambda l: \" \".join(x for x in l))"],"metadata":{"collapsed":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":["plsa = PLSA(data=gramrdd,sc=sc,k=5,max_itr=10,is_test=True)"],"metadata":{"collapsed":true,"scrolled":false},"outputs":[],"execution_count":12},{"cell_type":"code","source":["plsa.train()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["word_given_topic, topic_given_doc = plsa.save()"],"metadata":{"collapsed":true},"outputs":[],"execution_count":14},{"cell_type":"code","source":["topic_word = pd.DataFrame([sub.split(\" \") for sub in word_given_topic])\ntopic_word_1 = topic_word[topic_word.columns[:6]]\ntopic_word_1 = topic_word_1.set_index(0).astype('float')"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["doc_topic = pd.DataFrame([sub.split(\" \") for sub in topic_given_doc])\ndoc_topic_1 = doc_topic[doc_topic.columns[:6]]\ndoc_topic_1 = doc_topic_1.astype('float')"],"metadata":{"collapsed":true},"outputs":[],"execution_count":16},{"cell_type":"code","source":["doc_topic_1.head(10)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["topic_word_1.sort_values(topic_word_1.columns[4],ascending=False)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["topic_word.sort_values(topic_word.columns[2],ascending=False)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["topic_word.sort_values(topic_word.columns[3],ascending=False)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["topic_word.sort_values(topic_word.columns[4],ascending=False)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["topic_word.sort_values(topic_word.columns[5],ascending=False)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["topic_given_doc_1 = [x.split(\" \") for x in topic_given_doc]\ntopic_given_doc_2 = [[float(y) for y in x]for x in topic_given_doc_1]"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["from scipy.spatial import distance\n\nsome_pt = topic_given_doc_2[0]\nmin_index = distance.cdist([some_pt], topic_given_doc_2)[0].argsort()\n#topic_given_doc_2[min_index]"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["print(min_index[1:6])"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["print(len(topic_given_doc_2))"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":[""],"metadata":{"collapsed":true},"outputs":[],"execution_count":28}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.3","nbconvert_exporter":"python","file_extension":".py"},"name":"PLSA_topic_mining","notebookId":2974578033065903},"nbformat":4,"nbformat_minor":0}
